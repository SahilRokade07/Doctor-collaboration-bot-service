# Doctor Collaboration Bot Service

An intelligent medical assistant service that leverages local Large Language Models (LLMs) via Ollama to provide medical information, analyze documents, and assist healthcare professionals in their decision-making process. The service implements Retrieval-Augmented Generation (RAG) capabilities to enhance response accuracy and provide contextual medical insights.

## ğŸ¥ Overview

The Doctor Collaboration Bot Service is a FastAPI-based microservice designed specifically for healthcare professionals. It provides:

- **Medical Query Processing**: Intelligent responses to medical questions using specialized medical LLMs
- **PDF Document Analysis**: Automated extraction, summarization, and topic identification from medical documents
- **RAG Implementation**: Context-aware responses using document knowledge base
- **Conversation History**: Persistent storage of interactions for reference and analysis
- **RESTful API**: Easy integration with existing healthcare systems

## ğŸš€ Features

### Core Capabilities
- **Local LLM Integration**: Uses Ollama to run medical LLMs locally (MedLLaMA2, Meditron)
- **Medical Document Processing**: Extracts text from PDFs and generates intelligent summaries
- **Topic Extraction**: Automatically identifies key medical topics from documents
- **Query Context**: Supports contextual queries with additional medical information
- **Data Persistence**: JSON-based storage for queries, responses, and document summaries
- **Medical Text Processing**: Specialized text cleaning and medical term extraction

### Technical Features
- **Async Processing**: Non-blocking operations for better performance
- **Error Handling**: Robust error handling with timeout management
- **CORS Support**: Cross-origin resource sharing for web applications
- **Docker Support**: Containerized deployment ready
- **Type Safety**: Comprehensive Pydantic schemas for data validation
- **Modular Architecture**: Clean separation of concerns with service-based design

## ğŸ—ï¸ Architecture

```
src/
â”œâ”€â”€ api/                    # API routes and endpoints
â”‚   â”œâ”€â”€ routes.py          # Main API routes
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ core/                   # Core configuration
â”‚   â”œâ”€â”€ config.py          # Application settings
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ db/                     # Database handlers
â”‚   â”œâ”€â”€ handlers.py        # JSON database operations
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ documents/              # Document processing
â”‚   â”œâ”€â”€ exporter.py        # Export utilities
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ models/                 # Model configurations
â”‚   â””â”€â”€ llm/               # LLM-specific configs
â”œâ”€â”€ schemas/                # Data validation schemas
â”‚   â”œâ”€â”€ bot.py             # Bot request/response schemas
â”‚   â”œâ”€â”€ pdf.py             # PDF processing schemas
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ services/               # Business logic services
â”‚   â”œâ”€â”€ bot_service.py     # LLM interaction service
â”‚   â”œâ”€â”€ pdf_service.py     # PDF processing service
â”‚   â”œâ”€â”€ json_db_service.py # Database service
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ utils/                  # Utility functions
â”‚   â”œâ”€â”€ text_cleaner.py    # Medical text processing
â”‚   â””â”€â”€ __init__.py
â””â”€â”€ main.py                # FastAPI application entry point
```

## ğŸ“‹ Prerequisites

- **Python 3.11+**
- **Ollama** installed and running
- **Medical LLM Model** (MedLLaMA2 or Meditron)
- **Docker** (optional, for containerized deployment)

## âš™ï¸ Installation

### 1. Clone the Repository
```bash
git clone <repository-url>
cd doctor_collab_bot_service
```

### 2. Install Dependencies
Using UV (recommended):
```bash
pip install uv
uv pip install -e .
```

Using pip:
```bash
pip install -e .
```

### 3. Install and Configure Ollama

#### Install Ollama:
```bash
# On Linux/macOS
curl -fsSL https://ollama.ai/install.sh | sh

# On Windows
# Download from https://ollama.ai/download
```

#### Pull Medical Models:
```bash
# Pull MedLLaMA2 model
ollama pull medllama2

# Or pull Meditron model
ollama pull meditron
```

#### Start Ollama Service:
```bash
ollama serve
```

### 4. Configure Environment

Create a `.env` file or modify `src/core/config.py`:

```python
# LLM Configuration
OLLAMA_API_URL = "http://localhost:11434"
LLM_MODEL_NAME = "medllama2"  # or "meditron"

# Database Configuration
JSON_DB_PATH = "db/data.json"

# Security Configuration
SECRET_KEY = "your-secure-secret-key-here"
```

## ğŸš€ Usage

### Starting the Service

#### Development Mode:
```bash
uvicorn src.main:app --reload --host 0.0.0.0 --port 8000
```

#### Production Mode:
```bash
uvicorn src.main:app --host 0.0.0.0 --port 8000
```

#### Using Docker:
```bash
# Build the image
docker build -t doctor-collab-bot .

# Run the container
docker run -p 8000:8000 doctor-collab-bot
```

### API Documentation

Once running, access the interactive API documentation at:
- **Swagger UI**: http://localhost:8000/docs
- **ReDoc**: http://localhost:8000/redoc

## ğŸ“ API Endpoints

### 1. Medical Query Processing

**POST** `/query`

Process medical queries with contextual information.

```json
{
  "query": "What are the symptoms of hypertension?",
  "context": "Patient is 45-year-old male with family history of cardiovascular disease"
}
```

**Response:**
```json
{
  "response": "Hypertension symptoms include headaches, shortness of breath, nosebleeds...",
  "confidence": 0.95,
  "sources": []
}
```

### 2. PDF Document Upload

**POST** `/upload-pdf`

Upload and analyze medical PDF documents.

```bash
curl -X POST "http://localhost:8000/upload-pdf" \
     -F "file=@medical_document.pdf"
```

**Response:**
```json
{
  "filename": "medical_document.pdf",
  "page_count": 5,
  "summary": "This document discusses cardiovascular disease management...",
  "topics": ["hypertension", "cardiovascular disease", "treatment protocols"]
}
```

## ğŸ”§ Configuration

### LLM Models

The service supports multiple medical LLMs:

- **MedLLaMA2**: Specialized for medical question answering
- **Meditron**: Focused on clinical decision support

Configure in `src/core/config.py`:
```python
LLM_MODEL_NAME = "medllama2"  # or "meditron"
```

### Database Configuration

The service uses a JSON-based database for development. For production, consider integrating with:
- PostgreSQL
- MongoDB
- Vector databases (for enhanced RAG)

## ğŸ”’ Security Considerations

- **Data Privacy**: Ensure HIPAA compliance for medical data
- **Access Control**: Implement authentication for production use
- **Audit Logging**: Track all medical queries and responses
- **Data Encryption**: Encrypt sensitive medical information

## ğŸ§ª Testing

Run tests using pytest:
```bash
# Install development dependencies
uv pip install -e ".[dev]"

# Run tests
pytest

# Run with coverage
pytest --cov=src tests/
```

## ğŸ“Š Monitoring and Logging

The service includes basic logging. For production:

1. **Add structured logging**
2. **Implement health checks**
3. **Monitor LLM response times**
4. **Track query patterns**

## ğŸ”„ RAG Implementation

The service implements RAG through:

1. **Document Ingestion**: PDF processing and text extraction
2. **Context Retrieval**: Relevant document sections for queries
3. **Enhanced Responses**: LLM responses augmented with document context
4. **Knowledge Base**: Persistent storage of processed documents

### Enhancing RAG Capabilities

For advanced RAG implementation:

```python
# Add vector embeddings
pip install sentence-transformers chromadb

# Implement semantic search
# Add document chunking
# Create embedding store
```

## ğŸš§ Development Roadmap

### Planned Features
- [ ] Vector database integration for improved RAG
- [ ] Multi-modal support (images, DICOM files)
- [ ] Authentication and authorization
- [ ] Advanced medical entity recognition
- [ ] Integration with medical terminology APIs
- [ ] Real-time collaboration features
- [ ] Clinical decision support tools

### Performance Improvements
- [ ] Response caching
- [ ] Model optimization
- [ ] Batch processing
- [ ] Load balancing

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## âš ï¸ Disclaimer

This tool is designed to assist healthcare professionals and should not be used as a substitute for professional medical advice, diagnosis, or treatment. Always consult with qualified healthcare providers for medical decisions.

## ğŸ“ Support

For support and questions:
- Create an issue in the GitHub repository
- Contact the development team
- Review the API documentation

---

## ğŸ”— Related Technologies

- **FastAPI**: Modern web framework for building APIs
- **Ollama**: Local LLM runtime
- **Pydantic**: Data validation and settings management
- **PyPDF2**: PDF processing library
- **Uvicorn**: ASGI server implementation

Built with â¤ï¸ for healthcare professionals
